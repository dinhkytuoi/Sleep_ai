import sys, os, numpy as np, scipy.signal, tensorflow as tf
from sklearn.metrics import classification_report, f1_score
from sklearn.utils import shuffle, class_weight 
from TrainLSTM6lop import (load_trained_model_for_inference, load_single_subject,
                           CONFIG, augment_signal, focal_loss, SEED)
from collections import Counter

def run_finetuning_for_subject(sub, base_model_path):
    """
    H√†m ƒë·ªÉ fine-tune m·ªôt model cho m·ªôt subject c·ª• th·ªÉ.
    Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n c·ªßa model ƒë√£ ƒë∆∞·ª£c fine-tune.
    """
    model = load_trained_model_for_inference(base_model_path)

    # load subject
    X_raw, y = load_single_subject(sub)
    if X_raw is None:
        print(f"‚ùå Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu cho subject {sub}.")
        return None

    # TH√äM: Ki·ªÉm tra n·∫øu d·ªØ li·ªáu qu√° √≠t ƒë·ªÉ fine-tune
    MIN_EPOCHS_FOR_FINETUNE = 200 # Ng∆∞·ª°ng t·ªëi thi·ªÉu, c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh
    if X_raw.shape[0] < MIN_EPOCHS_FOR_FINETUNE:
        print(f"‚ö†Ô∏è D·ªØ li·ªáu c·ªßa subject {sub} qu√° √≠t ({X_raw.shape[0]} epochs). B·ªè qua fine-tuning ƒë·ªÉ tr√°nh l√†m gi·∫£m ch·∫•t l∆∞·ª£ng model.")
        return base_model_path # Tr·∫£ v·ªÅ model g·ªëc

    X_list=[]
    for i in range(X_raw.shape[0]):
        x = X_raw[i].astype(np.float32)
        x_r = scipy.signal.resample(x, CONFIG.TARGET_LENGTH_LSTM, axis=0).astype(np.float32)
        mean = x_r.mean(axis=0, keepdims=True); std = x_r.std(axis=0, keepdims=True)+1e-8
        X_list.append((x_r-mean)/std)
    X = np.stack(X_list).astype(np.float32)
    y = np.array(y).astype(int)

    # Oversample rare classes (N1, REM, Wake if needed)
    cnt = Counter(y.tolist())
    print("Before oversample counts:", cnt)
    target_min = max( int(np.percentile(list(cnt.values()), 50)), 30 ) 
    X_aug, y_aug = [], []
    
    # S·ª¨A L·ªñI LOGIC: T√≠nh to√°n `reps` ƒë·ªÉ tƒÉng c∆∞·ªùng c√°c l·ªõp thi·ªÉu s·ªë m·ªôt c√°ch ch√≠nh x√°c
    unique_classes, counts = zip(*cnt.items())
    median_count = np.median(counts)

    for cls in unique_classes:
        idxs = np.where(y==cls)[0]
        num_samples = len(idxs)
        # Ch·ªâ oversample c√°c l·ªõp c√≥ s·ªë l∆∞·ª£ng m·∫´u √≠t h∆°n m·ª©c trung v·ªã
        reps = int(np.ceil(median_count / num_samples)) if num_samples < median_count and num_samples > 0 else 1

        for r in range(reps):
            for i in idxs:
                xsel = X[i].copy()
                # small augmentation for odd reps
                if r>0:
                    xsel = augment_signal(xsel)
                X_aug.append(xsel)
                y_aug.append(cls)
    X_aug = np.stack(X_aug).astype(np.float32)
    y_aug = np.array(y_aug).astype(int)
    print("After oversample counts:", Counter(y_aug.tolist()))

    # S·ª¨A L·ªñI QUAN TR·ªåNG: X√°o tr·ªôn d·ªØ li·ªáu tr∆∞·ªõc khi chia validation_split
    X_aug, y_aug = shuffle(X_aug, y_aug, random_state=SEED)
    print("‚úÖ ƒê√£ x√°o tr·ªôn d·ªØ li·ªáu fine-tuning.")

    # TH√äM LOGIC: T√≠nh v√† √°p d·ª•ng Class Weights ƒë·ªÉ x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng l·ªõp
    classes = np.unique(y) 
    weights = class_weight.compute_class_weight(
        'balanced', classes=classes, y=y # T√≠nh tr√™n d·ªØ li·ªáu g·ªëc (y)
    )
    class_weights_dict = dict(zip(classes, weights))

    # ƒêi·ªÅu ch·ªânh n·∫øu l·ªõp Noise (nh√£n 5) c√≥ tr·ªçng s·ªë qu√° cao do √≠t m·∫´u
    if 5 in class_weights_dict:
        # N·∫øu Noise chi·∫øm d∆∞·ªõi 1% t·ªïng m·∫´u, ta gi·∫£m tr·ªçng s·ªë c·ªßa n√≥ ƒë·ªÉ tr√°nh h·ªçc l·ªách
        if (y == 5).sum() / len(y) < 0.01:
            # Gi·∫£m tr·ªçng s·ªë c·ªßa Noise (5) xu·ªëng t·ªëi ƒëa 1.0 
            class_weights_dict[5] = min(class_weights_dict[5], 1.0) 
            
    print("Class Weights cho fine-tuning:", class_weights_dict)
    # K·∫æT TH√öC LOGIC TH√äM V√ÄO

    # prepare labels
    n_out = model.output_shape[-1]
    y_cat = tf.keras.utils.to_categorical(y_aug, num_classes=n_out)

    # unfreeze more layers
    for layer in model.layers:
        layer.trainable = True

    # compile with focal loss
    # üìå ƒê√É TƒÇNG LEARNING RATE L√äN 2e-5 (TƒÉng g·∫•p ƒë√¥i so v·ªõi 1e-5 tr∆∞·ªõc ƒë√≥)
    opt = tf.keras.optimizers.Adam(learning_rate=2e-5) 
    model.compile(optimizer=opt, loss=focal_loss(gamma=2.0), metrics=['accuracy'])

    # callbacks
    cb = [
        # üìå ƒê√É TƒÇNG PATIENCE L√äN 10 ƒë·ªÉ cho model c√≥ th√™m c∆° h·ªôi c·∫£i thi·ªán
        tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)
    ]

    # fit
    history = model.fit(
        X_aug, y_cat, epochs=50, batch_size=16, 
        validation_split=0.1, callbacks=cb, verbose=1,
        class_weight=class_weights_dict # √Åp d·ª•ng Class Weights
    )

    out_path = f"fine_tuned_v2_{sub}.keras"
    model.save(out_path)
    print("Saved", out_path)

    # eval on full subject
    preds = np.argmax(model.predict(X), axis=1)
    print("Macro F1 after fine-tune v2:", f1_score(y, preds, average='macro', zero_division=0))
    # S·ª¨A L·ªñI: Cung c·∫•p tham s·ªë `labels` ƒë·ªÉ x·ª≠ l√Ω tr∆∞·ªùng h·ª£p subject thi·∫øu m·ªôt v√†i l·ªõp
    print(classification_report(
        y, preds,
        labels=list(range(n_out)),
        target_names=CONFIG.SLEEP_STAGE_LABELS[:n_out],
        zero_division=0
    ))
    
    return out_path

if __name__ == "__main__":
    sub_main = sys.argv[1] if len(sys.argv)>1 else input("subject: ")
    model_path_main = open("best_model_path.txt").read().strip()
    run_finetuning_for_subject(sub_main, model_path_main)