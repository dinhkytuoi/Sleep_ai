import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import joblib
import os
import glob
from datetime import datetime

import mne
from scipy.signal import butter, lfilter, welch
from scipy.stats import entropy

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import learning_curve, GroupKFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, cohen_kappa_score
from sklearn.preprocessing import StandardScaler

# ==================================
# âš™ï¸ CONFIGURATION
# ==================================

class CONFIG:
    SEED = 42
    RAW_DATA_DIR = r"A:\lstm+cnn\sleep-edf-database-expanded-1.0.0\sleep-edf-database-expanded-1.0.0\sleep-cassette"
    PROCESSED_DATA_DIR = "./processed_data_5_class"

    MODEL_DIR = "./saved_models_rf_class_weight"
    PLOTS_DIR = "./visualization_plots_rf_class_weight"

    SLEEP_STAGE_LABELS = ["Wake", "N1", "N2", "N3", "REM"]
    ANNOTATION_MAP = {
        "Sleep stage W": 0, "Sleep stage 1": 1, "Sleep stage 2": 2,
        "Sleep stage 3": 3, "Sleep stage 4": 3,
        "Sleep stage R": 4, "Sleep stage ?": -1, "Movement time": -1
    }

    EPOCH_DURATION_S = 30
    FREQ_BANDS = {"delta": [0.5, 4], "theta": [4, 8], "alpha": [8, 12], "sigma": [12, 16], "beta": [16, 30]}

# ==================================
# ğŸ”¬ DATA PREPROCESSING
# ==================================

def bandpass_filter(data, lowcut, highcut, fs, order=5):
    nyq = 0.5 * fs
    low, high = lowcut / nyq, highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return lfilter(b, a, data)

def get_spectral_features(epoch_data, fs):
    freqs, psd = welch(epoch_data, fs=fs, nperseg=fs*2)
    band_powers = []
    for band in CONFIG.FREQ_BANDS.values():
        idx_band = np.logical_and(freqs >= band[0], freqs <= band[1])
        band_powers.append(np.sum(psd[idx_band]))
    psd_norm = psd / np.sum(psd) if np.sum(psd) > 0 else psd
    spectral_entropy = entropy(psd_norm)
    return band_powers + [spectral_entropy]

def hjorth_parameters(epoch_data):
    activity = np.var(epoch_data)
    diff1 = np.diff(epoch_data)
    diff2 = np.diff(diff1)
    mobility = np.sqrt(np.var(diff1) / activity) if activity > 0 else 0
    complexity = np.sqrt(np.var(diff2) / np.var(diff1)) / mobility if np.var(diff1) > 0 and mobility > 0 else 0
    return [mobility, complexity]

def extract_features(epoch_data, fs):
    stat_features = [np.std(epoch_data), np.ptp(epoch_data)]
    spectral_features = get_spectral_features(epoch_data, fs)
    hjorth_features = hjorth_parameters(epoch_data)
    return stat_features + spectral_features + hjorth_features

def preprocess_raw_edf_data(raw_data_path):
    print("ğŸ”¬ Starting raw EDF data preprocessing (5 Lá»›p)...")
    psg_files = sorted(glob.glob(os.path.join(raw_data_path, "*PSG.edf")))
    hypno_files = sorted(glob.glob(os.path.join(raw_data_path, "*Hypnogram.edf")))
    if not psg_files or not hypno_files or len(psg_files) != len(hypno_files): return None, None, None

    all_features, all_labels, all_subject_ids = [], [], []
    for psg_filepath, hypno_filepath in zip(psg_files, hypno_files):
        subject_id = os.path.basename(psg_filepath).split('-')[0]
        print(f"   -> Processing subject: {subject_id}")
        raw = mne.io.read_raw_edf(psg_filepath, preload=True, verbose='WARNING')
        annot = mne.read_annotations(hypno_filepath)
        raw.set_annotations(annot, emit_warning=False)
        eeg_channel, fs = 'EEG Fpz-Cz', int(raw.info['sfreq'])
        eeg_data = raw.get_data(picks=[eeg_channel])[0]
        eeg_filtered = bandpass_filter(eeg_data, lowcut=0.5, highcut=45.0, fs=fs)
        events, _ = mne.events_from_annotations(raw, event_id=CONFIG.ANNOTATION_MAP, chunk_duration=CONFIG.EPOCH_DURATION_S)

        for event in events:
            start_sample, _, label = event
            if label == -1: continue
            end_sample = start_sample + CONFIG.EPOCH_DURATION_S * fs
            if end_sample > len(eeg_filtered): continue
            epoch_segment = eeg_filtered[start_sample:end_sample]
            features = extract_features(epoch_segment, fs)
            all_features.append(features)
            all_labels.append(label)
            all_subject_ids.append(subject_id)

    print("âœ… Raw data preprocessing complete.")
    return np.array(all_features), np.array(all_labels), np.array(all_subject_ids)

def load_data():
    os.makedirs(CONFIG.PROCESSED_DATA_DIR, exist_ok=True)
    X_path = os.path.join(CONFIG.PROCESSED_DATA_DIR, "X_features.npy")
    y_path = os.path.join(CONFIG.PROCESSED_DATA_DIR, "y_labels.npy")
    subjects_path = os.path.join(CONFIG.PROCESSED_DATA_DIR, "subject_ids.npy")
    try:
        print(f"ğŸ“¥ Attempting to load pre-processed 5-class data from {CONFIG.PROCESSED_DATA_DIR}...")
        X, y, subject_ids = np.load(X_path), np.load(y_path), np.load(subjects_path)
        print(f"âœ… Pre-processed 5-class data loaded successfully: X{X.shape}, y{y.shape}")
        return X, y, subject_ids
    except FileNotFoundError:
        print(f"âŒ Pre-processed 5-class data not found in {CONFIG.PROCESSED_DATA_DIR}. Processing from raw EDF files...")
        X, y, subject_ids = preprocess_raw_edf_data(CONFIG.RAW_DATA_DIR)
        if X is not None and len(X) > 0:
            print("ğŸ’¾ Saving 5-class processed data for future use...")
            np.save(X_path, X); np.save(y_path, y); np.save(subjects_path, subject_ids)
            print("âœ… Processed 5-class data saved.")
        return X, y, subject_ids

# ==================================
# ğŸ¤– MODEL TRAINING (LÆ¯U MODEL)
# ==================================

def train_random_forest_model(X, y, subject_ids):
    print("ğŸ¤– Training Optimized Random Forest (5 Lá»›p, 2/3 Train, 1/3 Test, Class Weight)")

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    unique_subjects = np.unique(subject_ids)
    if len(unique_subjects) < 2:
        print("âŒ Cáº§n Ã­t nháº¥t 2 subject Ä‘á»ƒ chia train/test.")
        return [None]*11

    np.random.seed(CONFIG.SEED)
    np.random.shuffle(unique_subjects)

    split_idx = int(len(unique_subjects) * 2 / 3)
    train_subjects = unique_subjects[:split_idx]
    test_subjects = unique_subjects[split_idx:]

    print(f"   -> Tá»•ng sá»‘ subjects: {len(unique_subjects)}")
    print(f"   -> Subjects huáº¥n luyá»‡n (2/3): {len(train_subjects)}")
    print(f"   -> Subjects kiá»ƒm tra (1/3): {len(test_subjects)}")

    train_mask = np.isin(subject_ids, train_subjects)
    test_mask = np.isin(subject_ids, test_subjects)

    X_train, y_train = X_scaled[train_mask], y[train_mask]
    X_test, y_test = X_scaled[test_mask], y[test_mask] # X_test Ä‘Æ°á»£c táº¡o á»Ÿ Ä‘Ã¢y
    groups_train = subject_ids[train_mask]

    print(f"   -> Tá»•ng sá»‘ epoch: {len(X_scaled)}")
    print(f"   -> Epochs huáº¥n luyá»‡n: {len(X_train)}")
    print(f"   -> Epochs kiá»ƒm tra: {len(X_test)}")
    print("   -> Sá»­ dá»¥ng 'class_weight=balanced' Ä‘á»ƒ xá»­ lÃ½ máº¥t cÃ¢n báº±ng.")

    print("ğŸŒ³ Training model...")
    model = RandomForestClassifier(n_estimators=200,
                                   random_state=CONFIG.SEED,
                                   n_jobs=-1,
                                   max_depth=25,
                                   min_samples_leaf=5,
                                   class_weight='balanced'
                                  )
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)
    kappa = cohen_kappa_score(y_test, y_pred)

    print(f"\nğŸ¯ Model Performance (trÃªn 1/3 test subjects):")
    print(f"   Accuracy: {accuracy:.4f}")
    print(f"   F1-Score (Macro): {f1:.4f}")
    print(f"   Cohen's Kappa: {kappa:.4f}")

    print("\nğŸ’¾ Saving model, scaler, and subject lists...")
    os.makedirs(CONFIG.MODEL_DIR, exist_ok=True)

    joblib.dump(model, os.path.join(CONFIG.MODEL_DIR, "rf_model.joblib"))
    joblib.dump(scaler, os.path.join(CONFIG.MODEL_DIR, "rf_scaler.joblib"))
    np.save(os.path.join(CONFIG.MODEL_DIR, "rf_test_subjects.npy"), test_subjects)

    print(f"âœ… Model vÃ  data Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o {CONFIG.MODEL_DIR}")

    return model, scaler, y_test, y_pred, accuracy, f1, kappa, X_train, y_train, groups_train, X_test

# ==================================
# ğŸ“Š VISUALIZATION
# ==================================

def plot_stage_distribution(y_data, title, filename):
    plt.figure(figsize=(10, 6))
    unique_ticks = sorted(np.unique(y_data))
    unique_labels = [CONFIG.SLEEP_STAGE_LABELS[i] for i in unique_ticks if i < len(CONFIG.SLEEP_STAGE_LABELS)]
    sns.countplot(x=y_data, order=unique_ticks)
    plt.title(f'PhÃ¢n bá»‘ giai Ä‘oáº¡n - {title}', fontsize=16)
    plt.xlabel('Giai Ä‘oáº¡n ngá»§', fontsize=12)
    plt.ylabel('Sá»‘ lÆ°á»£ng Epochs', fontsize=12)
    plt.xticks(ticks=unique_ticks, labels=unique_labels)
    save_path = os.path.join(CONFIG.PLOTS_DIR, filename)
    plt.savefig(save_path, bbox_inches='tight')
    plt.close()
    print(f"   -> ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ phÃ¢n bá»‘: {filename}")

def plot_confusion_matrix(y_true, y_pred, filename):
    cm = confusion_matrix(y_true, y_pred)
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm_percent, annot=True, fmt='.2%', cmap='Blues',
                xticklabels=CONFIG.SLEEP_STAGE_LABELS,
                yticklabels=CONFIG.SLEEP_STAGE_LABELS)
    plt.title('Ma tráº­n nháº§m láº«n (Táº­p Test - Chuáº©n hÃ³a theo Recall)', fontsize=16)
    plt.xlabel('NhÃ£n dá»± Ä‘oÃ¡n', fontsize=12)
    plt.ylabel('NhÃ£n thá»±c táº¿', fontsize=12)
    save_path = os.path.join(CONFIG.PLOTS_DIR, filename)
    plt.savefig(save_path, bbox_inches='tight')
    plt.close()
    print(f"   -> ÄÃ£ lÆ°u ma tráº­n nháº§m láº«n: {filename}")

def plot_per_stage_performance(y_true, y_pred, filename):
    report = classification_report(y_true, y_pred, target_names=CONFIG.SLEEP_STAGE_LABELS,
                                   output_dict=True, zero_division=0)
    df = pd.DataFrame(report).T
    df = df.drop(['accuracy', 'macro avg', 'weighted avg'])
    df = df.drop(columns=['support'])
    df.plot(kind='bar', figsize=(12, 7), rot=0)
    plt.title('Hiá»‡u suáº¥t chi tiáº¿t tá»«ng giai Ä‘oáº¡n (Táº­p Test)', fontsize=16)
    plt.xlabel('Giai Ä‘oáº¡n ngá»§', fontsize=12)
    plt.ylabel('Äiá»ƒm sá»‘ (0.0 - 1.0)', fontsize=12)
    plt.legend(loc='lower right')
    save_path = os.path.join(CONFIG.PLOTS_DIR, filename)
    plt.savefig(save_path, bbox_inches='tight')
    plt.close()
    print(f"   -> ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ hiá»‡u suáº¥t: {filename}")

def plot_f1_kappa_comparison(f1, kappa, filename):
    plt.figure(figsize=(8, 6))
    sns.barplot(x=['Macro F1-Score', "Cohen's Kappa"], y=[f1, kappa])
    plt.title('So sÃ¡nh chá»‰ sá»‘ tá»•ng quÃ¡t (Táº­p Test)', fontsize=16)
    plt.ylabel('Äiá»ƒm sá»‘', fontsize=12)
    plt.ylim(0, 1.0)
    save_path = os.path.join(CONFIG.PLOTS_DIR, filename)
    plt.savefig(save_path, bbox_inches='tight')
    plt.close()
    print(f"   -> ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ F1-Kappa: {filename}")

def plot_learning_curve(estimator, X_train, y_train, groups_train, filename):
    """Váº½ Ä‘Æ°á»ng cong há»c táº­p."""
    print("   -> Báº¯t Ä‘áº§u váº½ Ä‘Æ°á»ng cong há»c táº­p...")

    n_splits_cv = min(3, len(np.unique(groups_train)))
    train_sizes_abs = np.linspace(0.2, 1.0, 3)

    if n_splits_cv < 2:
        print("   -> KhÃ´ng Ä‘á»§ nhÃ³m (subject) Ä‘á»ƒ thá»±c hiá»‡n CV. Bá» qua váº½ Ä‘Æ°á»ng cong há»c táº­p.")
        return

    print(f"   -> Cháº¡y learning_curve vá»›i {n_splits_cv} folds vÃ  {len(train_sizes_abs)} kÃ­ch thÆ°á»›c.")

    try:
        cv = GroupKFold(n_splits=n_splits_cv)
        train_sizes, train_scores, valid_scores = learning_curve(
            estimator=estimator,
            X=X_train, y=y_train, groups=groups_train,
            cv=cv, n_jobs=-1,
            train_sizes=train_sizes_abs,
            scoring='f1_macro',
            error_score='raise'
        )

        train_scores_mean = np.mean(train_scores, axis=1)
        train_scores_std = np.std(train_scores, axis=1)
        valid_scores_mean = np.mean(valid_scores, axis=1)
        valid_scores_std = np.std(valid_scores, axis=1)

        plt.figure(figsize=(12, 8))
        plt.title('ÄÆ°á»ng cong há»c táº­p (Learning Curve - DÃ¹ng Class Weight)', fontsize=16)
        plt.xlabel('Sá»‘ lÆ°á»£ng máº«u huáº¥n luyá»‡n', fontsize=12)
        plt.ylabel('F1-Score (Macro)', fontsize=12)
        plt.grid()
        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                         train_scores_mean + train_scores_std, alpha=0.1, color='r')
        plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,
                         valid_scores_mean + valid_scores_std, alpha=0.1, color='g')
        plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Äiá»ƒm huáº¥n luyá»‡n (Training score)')
        plt.plot(train_sizes, valid_scores_mean, 'o-', color='g', label='Äiá»ƒm kiá»ƒm Ä‘á»‹nh (Cross-validation score)')
        plt.legend(loc='best')
        save_path = os.path.join(CONFIG.PLOTS_DIR, filename)
        plt.savefig(save_path, bbox_inches='tight')
        plt.close()
        print(f"   -> ÄÃ£ lÆ°u Ä‘Æ°á»ng cong há»c táº­p: {filename}")

    except Exception as e:
        print(f"   -> âŒ Lá»–I khi váº½ Ä‘Æ°á»ng cong há»c táº­p: {e}")
        print("   -> Bá» qua viá»‡c váº½ biá»ƒu Ä‘á»“ nÃ y.")

# --- Báº®T Äáº¦U PHáº¦N Má»šI ---
def plot_rf_training_dynamics(X_train, y_train, X_test, y_test, filename):
    """
    Váº½ biá»ƒu Ä‘á»“ Accuracy vÃ  F1-Score (trÃªn táº­p Test)
    khi tÄƒng dáº§n sá»‘ lÆ°á»£ng cÃ¢y (n_estimators)
    nháº±m mÃ´ phá»ng quÃ¡ trÃ¬nh há»™i tá»¥ cá»§a mÃ´ hÃ¬nh.
    """
    print("   -> Äang váº½ biá»ƒu Ä‘á»“ quÃ¡ trÃ¬nh há»™i tá»¥ (Accuracy & F1 theo sá»‘ lÆ°á»£ng cÃ¢y)...")
    # Giáº£m sá»‘ Ä‘iá»ƒm Ä‘á»ƒ cháº¡y nhanh hÆ¡n (tá»« 10 Ä‘áº¿n 200, 10 Ä‘iá»ƒm)
    n_estimators_list = np.linspace(10, 200, 10, dtype=int)
    acc_scores, f1_scores = [], []

    for n in n_estimators_list:
        # Huáº¥n luyá»‡n model táº¡m thá»i vá»›i sá»‘ lÆ°á»£ng cÃ¢y 'n'
        model_temp = RandomForestClassifier(
            n_estimators=n, random_state=CONFIG.SEED, n_jobs=-1,
            max_depth=25, min_samples_leaf=5, class_weight='balanced'
        )
        model_temp.fit(X_train, y_train)
        y_pred = model_temp.predict(X_test) # ÄÃ¡nh giÃ¡ trÃªn táº­p Test
        acc_scores.append(accuracy_score(y_test, y_pred))
        f1_scores.append(f1_score(y_test, y_pred, average='macro', zero_division=0))

    # Váº½ biá»ƒu Ä‘á»“
    plt.figure(figsize=(10, 6))
    plt.plot(n_estimators_list, acc_scores, 'o-', label='Accuracy (trÃªn táº­p Test)', color='steelblue')
    plt.plot(n_estimators_list, f1_scores, 's--', label='Macro F1-Score (trÃªn táº­p Test)', color='darkorange')
    plt.title('QuÃ¡ trÃ¬nh há»™i tá»¥ theo sá»‘ lÆ°á»£ng cÃ¢y (Random Forest)', fontsize=16)
    plt.xlabel('Sá»‘ lÆ°á»£ng cÃ¢y (n_estimators)', fontsize=12)
    plt.ylabel('Äiá»ƒm sá»‘ (trÃªn táº­p Test)', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()
    save_path = os.path.join(CONFIG.PLOTS_DIR, filename)
    plt.savefig(save_path, bbox_inches='tight')
    plt.close()
    print(f"   -> ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ quÃ¡ trÃ¬nh há»™i tá»¥: {filename}")


def plot_prediction_confidence_distribution(model, X_test, y_test, filename):
    """
    Váº½ phÃ¢n bá»‘ xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cao nháº¥t trÃªn táº­p test (Ä‘á»™ tá»± tin cá»§a mÃ´ hÃ¬nh).
    """
    print("   -> Äang váº½ phÃ¢n bá»‘ xÃ¡c suáº¥t dá»± Ä‘oÃ¡n (prediction confidence)...")
    # Kiá»ƒm tra xem model cÃ³ há»— trá»£ predict_proba khÃ´ng
    if not hasattr(model, "predict_proba"):
        print("   -> Model khÃ´ng há»— trá»£ predict_proba, bá» qua.")
        return

    # Láº¥y xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho tá»«ng lá»›p
    y_proba = model.predict_proba(X_test)
    # Láº¥y xÃ¡c suáº¥t cao nháº¥t (Ä‘á»™ tá»± tin) cho má»—i dá»± Ä‘oÃ¡n
    max_probs = np.max(y_proba, axis=1)

    # Váº½ biá»ƒu Ä‘á»“ histogram
    plt.figure(figsize=(10, 6))
    sns.histplot(max_probs, bins=30, kde=True, color='royalblue')
    plt.title('PhÃ¢n bá»‘ xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cao nháº¥t trÃªn táº­p Test', fontsize=16)
    plt.xlabel('XÃ¡c suáº¥t dá»± Ä‘oÃ¡n cao nháº¥t (Äá»™ tá»± tin)', fontsize=12)
    plt.ylabel('Sá»‘ lÆ°á»£ng máº«u', fontsize=12)
    plt.grid(alpha=0.3)
    save_path = os.path.join(CONFIG.PLOTS_DIR, filename)
    plt.savefig(save_path, bbox_inches='tight')
    plt.close()
    print(f"   -> ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ phÃ¢n bá»‘ xÃ¡c suáº¥t: {filename}")
# --- Káº¾T THÃšC PHáº¦N Má»šI ---

# ==================================
# ğŸš€ MAIN PIPELINE (ÄÃƒ Cáº¬P NHáº¬T)
# ==================================
def main():
    print("ğŸš€ SLEEP STAGE CLASSIFICATION PIPELINE (5 Lá»šP, DÃ¹ng Class Weight)")
    print("=" * 60)

    os.makedirs(CONFIG.PLOTS_DIR, exist_ok=True)

    X, y, subject_ids = load_data()
    if X is None or len(X) == 0:
        print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xá»­ lÃ½. Dá»«ng pipeline.")
        return

    plot_stage_distribution(y, "ToÃ n bá»™ Dá»¯ liá»‡u (5 Lá»›p)", "1_distribution_full_dataset.png")

    results = train_random_forest_model(X, y, subject_ids)
    if results[0] is None:
        print("âŒ Huáº¥n luyá»‡n tháº¥t báº¡i.")
        return

    model, scaler, y_test, y_pred, acc, f1, kappa, X_train, y_train, groups_train, X_test = results

    print("\nğŸ“‹ DETAILED CLASSIFICATION REPORT (5 Lá»›p, Class Weight, trÃªn táº­p 1/3 test subjects):")
    print("=" * 60)
    print(classification_report(y_test, y_pred, target_names=CONFIG.SLEEP_STAGE_LABELS, digits=4, zero_division=0))
    print(f"Overall Accuracy: {acc:.4f}")
    print(f"Overall Macro F1-Score: {f1:.4f}")
    print(f"Overall Cohen's Kappa: {cohen_kappa_score(y_test, y_pred):.4f}")

    print("\nğŸ“Š Generating and saving visualization plots...")

    plot_stage_distribution(y_test, "Táº­p Test (5 Lá»›p, 1/3 Subjects)", "2_distribution_test_set.png")
    plot_confusion_matrix(y_test, y_pred, "3_confusion_matrix.png")
    plot_per_stage_performance(y_test, y_pred, "4_per_stage_performance.png")
    plot_f1_kappa_comparison(f1, kappa, "5_f1_vs_kappa.png")

    base_model_for_lc = RandomForestClassifier(
        n_estimators=200, random_state=CONFIG.SEED, n_jobs=-1,
        max_depth=25, min_samples_leaf=5, class_weight='balanced'
    )

    plot_learning_curve(base_model_for_lc, X_train, y_train, groups_train, "6_learning_curve.png")

    # === Gá»ŒI CÃC HÃ€M Váº¼ BIá»‚U Äá»’ Bá»” SUNG ===
    print("\nğŸ“Š Generating supplementary visualization plots...")
    # ChÃº Ã½: plot_rf_training_dynamics sáº½ cháº¡y hÆ¡i cháº­m vÃ¬ nÃ³ huáº¥n luyá»‡n láº¡i model 10 láº§n
    plot_rf_training_dynamics(X_train, y_train, X_test, y_test, "7_rf_training_dynamics.png")
    plot_prediction_confidence_distribution(model, X_test, y_test, "8_prediction_confidence_distribution.png")

    print(f"\nâœ… PIPELINE COMPLETED SUCCESSFULLY!")
    print(f"ğŸ“ˆ Táº¥t cáº£ 8 biá»ƒu Ä‘á»“ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o thÆ° má»¥c: {CONFIG.PLOTS_DIR}")
    print(f"ğŸ“¦ Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o thÆ° má»¥c: {CONFIG.MODEL_DIR}")

if __name__ == "__main__":
    main()