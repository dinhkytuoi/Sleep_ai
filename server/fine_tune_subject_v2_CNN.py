import sys, os, numpy as np, scipy.signal, tensorflow as tf
from sklearn.metrics import classification_report, f1_score
from sklearn.utils import shuffle, class_weight 
from TrainCNN6lop import (load_trained_model_for_inference, load_single_subject,
                           CONFIG, augment_signal, focal_loss, SEED)
from collections import Counter

def run_finetuning_for_subject(sub, base_model_path):
    """
    HÃ m Ä‘á»ƒ fine-tune má»™t model cho má»™t subject cá»¥ thá»ƒ.
    Tráº£ vá» Ä‘Æ°á»ng dáº«n cá»§a model Ä‘Ã£ Ä‘Æ°á»£c fine-tune.
    """
    model = load_trained_model_for_inference(base_model_path)

    # load subject
    X_raw, y = load_single_subject(sub)
    if X_raw is None:
        print(f"âŒ KhÃ´ng thá»ƒ táº£i dá»¯ liá»‡u cho subject {sub}.")
        return None

    # THÃŠM: Kiá»ƒm tra náº¿u dá»¯ liá»‡u quÃ¡ Ã­t Ä‘á»ƒ fine-tune
    MIN_EPOCHS_FOR_FINETUNE = 200 # NgÆ°á»¡ng tá»‘i thiá»ƒu, cÃ³ thá»ƒ Ä‘iá»u chá»‰nh
    if X_raw.shape[0] < MIN_EPOCHS_FOR_FINETUNE:
        print(f"âš ï¸ Dá»¯ liá»‡u cá»§a subject {sub} quÃ¡ Ã­t ({X_raw.shape[0]} epochs). Bá» qua fine-tuning Ä‘á»ƒ trÃ¡nh lÃ m giáº£m cháº¥t lÆ°á»£ng model.")
        return base_model_path # Tráº£ vá» model gá»‘c

    X_list=[]
    for i in range(X_raw.shape[0]):
        x = X_raw[i].astype(np.float32)
        x_r = scipy.signal.resample(x, CONFIG.TARGET_LENGTH_CNN, axis=0).astype(np.float32)
        mean = x_r.mean(axis=0, keepdims=True); std = x_r.std(axis=0, keepdims=True)+1e-8
        X_list.append((x_r-mean)/std)
    X = np.stack(X_list).astype(np.float32)
    y = np.array(y).astype(int)

    # Oversample rare classes (N1, REM, Wake if needed)
    cnt = Counter(y.tolist())
    print("Before oversample counts:", cnt)
    target_min = max( int(np.percentile(list(cnt.values()), 50)), 30 ) 
    X_aug, y_aug = [], []
    
    # Sá»¬A Lá»–I LOGIC: TÃ­nh toÃ¡n `reps` Ä‘á»ƒ tÄƒng cÆ°á»ng cÃ¡c lá»›p thiá»ƒu sá»‘ má»™t cÃ¡ch chÃ­nh xÃ¡c
    unique_classes, counts = zip(*cnt.items())
    median_count = np.median(counts)

    for cls in unique_classes:
        idxs = np.where(y==cls)[0]
        num_samples = len(idxs)
        # Chá»‰ oversample cÃ¡c lá»›p cÃ³ sá»‘ lÆ°á»£ng máº«u Ã­t hÆ¡n má»©c trung vá»‹
        reps = int(np.ceil(median_count / num_samples)) if num_samples < median_count and num_samples > 0 else 1

        for r in range(reps):
            for i in idxs:
                xsel = X[i].copy()
                # small augmentation for odd reps
                if r>0:
                    xsel = augment_signal(xsel)
                X_aug.append(xsel)
                y_aug.append(cls)
    X_aug = np.stack(X_aug).astype(np.float32)
    y_aug = np.array(y_aug).astype(int)
    print("After oversample counts:", Counter(y_aug.tolist()))

    # Sá»¬A Lá»–I QUAN TRá»ŒNG: XÃ¡o trá»™n dá»¯ liá»‡u trÆ°á»›c khi chia validation_split
    X_aug, y_aug = shuffle(X_aug, y_aug, random_state=SEED)
    print("âœ… ÄÃ£ xÃ¡o trá»™n dá»¯ liá»‡u fine-tuning.")

    # THÃŠM LOGIC: TÃ­nh vÃ  Ã¡p dá»¥ng Class Weights Ä‘á»ƒ xá»­ lÃ½ máº¥t cÃ¢n báº±ng lá»›p
    classes = np.unique(y) 
    weights = class_weight.compute_class_weight(
        'balanced', classes=classes, y=y # TÃ­nh trÃªn dá»¯ liá»‡u gá»‘c (y)
    )
    class_weights_dict = dict(zip(classes, weights))

    # Äiá»u chá»‰nh náº¿u lá»›p Noise (nhÃ£n 5) cÃ³ trá»ng sá»‘ quÃ¡ cao do Ã­t máº«u
    if 5 in class_weights_dict:
        # Náº¿u Noise chiáº¿m dÆ°á»›i 1% tá»•ng máº«u, ta giáº£m trá»ng sá»‘ cá»§a nÃ³ Ä‘á»ƒ trÃ¡nh há»c lá»‡ch
        if (y == 5).sum() / len(y) < 0.01:
            # Giáº£m trá»ng sá»‘ cá»§a Noise (5) xuá»‘ng tá»‘i Ä‘a 1.0 
            class_weights_dict[5] = min(class_weights_dict[5], 1.0) 
            
    print("Class Weights cho fine-tuning:", class_weights_dict)
    # Káº¾T THÃšC LOGIC THÃŠM VÃ€O

    # prepare labels
    n_out = model.output_shape[-1]
    y_cat = tf.keras.utils.to_categorical(y_aug, num_classes=n_out)

    # unfreeze more layers
    for layer in model.layers:
        layer.trainable = True

    # compile with focal loss
    # ğŸ“Œ ÄÃƒ TÄ‚NG LEARNING RATE LÃŠN 2e-5 (TÄƒng gáº¥p Ä‘Ã´i so vá»›i 1e-5 trÆ°á»›c Ä‘Ã³)
    opt = tf.keras.optimizers.Adam(learning_rate=2e-5) 
    model.compile(optimizer=opt, loss=focal_loss(gamma=2.0), metrics=['accuracy'])

    # callbacks
    cb = [
        # ğŸ“Œ ÄÃƒ TÄ‚NG PATIENCE LÃŠN 10 Ä‘á»ƒ cho model cÃ³ thÃªm cÆ¡ há»™i cáº£i thiá»‡n
        tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)
    ]

    # fit
    history = model.fit(
        X_aug, y_cat, epochs=50, batch_size=16, 
        validation_split=0.1, callbacks=cb, verbose=1,
        class_weight=class_weights_dict # Ãp dá»¥ng Class Weights
    )

    # Sá»¬A: LÆ°u model Ä‘Ã£ fine-tune vÃ o cÃ¹ng thÆ° má»¥c vá»›i model gá»‘c
    # Äiá»u nÃ y giÃºp analyze_sleep.py tÃ¬m tháº¥y nÃ³ dá»… dÃ ng hÆ¡n
    base_model_dir = os.path.dirname(base_model_path)
    out_path = os.path.join(base_model_dir, f"fine_tuned_v2_{sub}.keras")

    model.save(out_path, include_optimizer=False) # LÆ°u khÃ´ng cáº§n optimizer Ä‘á»ƒ file nháº¹ hÆ¡n
    print("Saved", out_path)

    # eval on full subject
    preds = np.argmax(model.predict(X), axis=1)
    print("Macro F1 after fine-tune v2:", f1_score(y, preds, average='macro', zero_division=0))
    # Sá»¬A Lá»–I: Cung cáº¥p tham sá»‘ `labels` Ä‘á»ƒ xá»­ lÃ½ trÆ°á»ng há»£p subject thiáº¿u má»™t vÃ i lá»›p
    print(classification_report(
        y, preds,
        labels=list(range(n_out)),
        target_names=CONFIG.SLEEP_STAGE_LABELS[:n_out],
        zero_division=0
    ))
    
    return out_path

if __name__ == "__main__":
    sub_main = sys.argv[1] if len(sys.argv)>1 else input("subject: ")
    model_path_main = open("best_model_path.txt").read().strip()
    run_finetuning_for_subject(sub_main, model_path_main)